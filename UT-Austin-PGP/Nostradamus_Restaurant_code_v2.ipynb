{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: opendatasets in c:\\users\\anirb\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (0.1.22)\n",
      "Requirement already satisfied: pandas in c:\\users\\anirb\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (1.5.3)\n",
      "Requirement already satisfied: numpy in c:\\users\\anirb\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (1.25.2)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\anirb\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (1.2.2)\n",
      "Requirement already satisfied: xgboost in c:\\users\\anirb\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (2.0.3)\n",
      "Requirement already satisfied: lightgbm in c:\\users\\anirb\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (4.3.0)\n",
      "Requirement already satisfied: tabulate in c:\\users\\anirb\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (0.9.0)\n",
      "Requirement already satisfied: category_encoders in c:\\users\\anirb\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (2.6.3)\n",
      "Requirement already satisfied: tqdm in c:\\users\\anirb\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from opendatasets) (4.66.4)\n",
      "Requirement already satisfied: kaggle in c:\\users\\anirb\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from opendatasets) (1.6.14)\n",
      "Requirement already satisfied: click in c:\\users\\anirb\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from opendatasets) (8.1.7)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\users\\anirb\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\anirb\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: scipy>=1.3.2 in c:\\users\\anirb\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from scikit-learn) (1.13.1)\n",
      "Requirement already satisfied: joblib>=1.1.1 in c:\\users\\anirb\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\anirb\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from scikit-learn) (3.5.0)\n",
      "Requirement already satisfied: statsmodels>=0.9.0 in c:\\users\\anirb\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from category_encoders) (0.14.2)\n",
      "Requirement already satisfied: patsy>=0.5.1 in c:\\users\\anirb\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from category_encoders) (0.5.6)\n",
      "Requirement already satisfied: six in c:\\users\\anirb\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from patsy>=0.5.1->category_encoders) (1.16.0)\n",
      "Requirement already satisfied: packaging>=21.3 in c:\\users\\anirb\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from statsmodels>=0.9.0->category_encoders) (24.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\anirb\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from click->opendatasets) (0.4.6)\n",
      "Requirement already satisfied: certifi>=2023.7.22 in c:\\users\\anirb\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from kaggle->opendatasets) (2024.2.2)\n",
      "Requirement already satisfied: requests in c:\\users\\anirb\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from kaggle->opendatasets) (2.32.2)\n",
      "Requirement already satisfied: python-slugify in c:\\users\\anirb\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from kaggle->opendatasets) (8.0.4)\n",
      "Requirement already satisfied: urllib3 in c:\\users\\anirb\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from kaggle->opendatasets) (2.2.1)\n",
      "Requirement already satisfied: bleach in c:\\users\\anirb\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from kaggle->opendatasets) (6.1.0)\n",
      "Requirement already satisfied: webencodings in c:\\users\\anirb\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from bleach->kaggle->opendatasets) (0.5.1)\n",
      "Requirement already satisfied: text-unidecode>=1.3 in c:\\users\\anirb\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from python-slugify->kaggle->opendatasets) (1.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\anirb\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from requests->kaggle->opendatasets) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\anirb\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from requests->kaggle->opendatasets) (3.7)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install numpy==1.25.2 pandas==1.5.3 matplotlib==3.7.1 seaborn==0.13.1 scikit-learn==1.2.2 sklearn-pandas==2.2.0 -q --user\n",
    "%pip install opendatasets pandas numpy scikit-learn xgboost lightgbm tabulate category_encoders\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-16T14:08:47.244579Z",
     "start_time": "2020-09-16T14:08:47.236547Z"
    }
   },
   "outputs": [],
   "source": [
    "#import some necessary librairies\n",
    "import datetime\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt  # Matlab-style plotting\n",
    "import seaborn as sns\n",
    "color = sns.color_palette()\n",
    "sns.set_style('darkgrid')\n",
    "import warnings\n",
    "def ignore_warn(*args, **kwargs):\n",
    "    pass\n",
    "warnings.warn = ignore_warn #ignore annoying warning (from sklearn and seaborn)\n",
    "\n",
    "\n",
    "from scipy import stats\n",
    "from scipy.stats import norm, skew #for some statistics\n",
    "\n",
    "\n",
    "pd.set_option('display.float_format', lambda x: '{:.3f}'.format(x)) #Limiting floats output to 3 decimal points\n",
    "\n",
    "\n",
    "from subprocess import check_output\n",
    "#print(check_output([\"dir\", \"C:/Users/anirb/aiml/UT-Austin-PGP/input/\"]).decode(\"utf8\")) #check the files available in the directory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading the Training Data and using the Training Data to build the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-16T13:19:51.066441Z",
     "start_time": "2020-09-16T13:19:50.965370Z"
    }
   },
   "outputs": [],
   "source": [
    "#Now let's import and put the train and test datasets in  pandas dataframe\n",
    "folderPath = 'C:/Users/anirb/aiml/UT-Austin-PGP'\n",
    "train = pd.read_csv(folderPath + '/input/train.csv')\n",
    "test = pd.read_csv(folderPath + '/input/test.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Registration Number</th>\n",
       "      <th>Cuisine</th>\n",
       "      <th>City</th>\n",
       "      <th>Restaurant Location</th>\n",
       "      <th>Opening Day of Restaurant</th>\n",
       "      <th>Facebook Popularity Quotient</th>\n",
       "      <th>Endorsed By</th>\n",
       "      <th>Instagram Popularity Quotient</th>\n",
       "      <th>Fire Audit</th>\n",
       "      <th>Liquor License Obtained</th>\n",
       "      <th>...</th>\n",
       "      <th>Overall Restaurant Rating</th>\n",
       "      <th>Live Music Rating</th>\n",
       "      <th>Comedy Gigs Rating</th>\n",
       "      <th>Value Deals Rating</th>\n",
       "      <th>Live Sports Rating</th>\n",
       "      <th>Ambience</th>\n",
       "      <th>Lively</th>\n",
       "      <th>Service</th>\n",
       "      <th>Comfortablility</th>\n",
       "      <th>Privacy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20001</td>\n",
       "      <td>tibetan,italian</td>\n",
       "      <td>Bangalore</td>\n",
       "      <td>Near Business Hub</td>\n",
       "      <td>13-07-2010</td>\n",
       "      <td>78.000</td>\n",
       "      <td>Tier A Celebrity</td>\n",
       "      <td>69.000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>6.000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20002</td>\n",
       "      <td>tibetan,italian</td>\n",
       "      <td>Hyderabad</td>\n",
       "      <td>Near Party Hub</td>\n",
       "      <td>05-09-2011</td>\n",
       "      <td>89.170</td>\n",
       "      <td>Not Specific</td>\n",
       "      <td>96.000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>9.000</td>\n",
       "      <td>3.000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6.000</td>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20003</td>\n",
       "      <td>algerian,belgian</td>\n",
       "      <td>Hyderabad</td>\n",
       "      <td>Near Party Hub</td>\n",
       "      <td>12-04-2011</td>\n",
       "      <td>84.000</td>\n",
       "      <td>Not Specific</td>\n",
       "      <td>86.000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>6.000</td>\n",
       "      <td>3.000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>20004</td>\n",
       "      <td>tibetan,greek</td>\n",
       "      <td>-1</td>\n",
       "      <td>Near Party Hub</td>\n",
       "      <td>16-01-2005</td>\n",
       "      <td>79.380</td>\n",
       "      <td>Not Specific</td>\n",
       "      <td>74.400</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>9.000</td>\n",
       "      <td>6.000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20005</td>\n",
       "      <td>cuban,british</td>\n",
       "      <td>Pune</td>\n",
       "      <td>Near Party Hub</td>\n",
       "      <td>10-11-2008</td>\n",
       "      <td>84.670</td>\n",
       "      <td>Not Specific</td>\n",
       "      <td>86.460</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 33 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Registration Number           Cuisine       City Restaurant Location  \\\n",
       "0                20001   tibetan,italian  Bangalore   Near Business Hub   \n",
       "1                20002   tibetan,italian  Hyderabad      Near Party Hub   \n",
       "2                20003  algerian,belgian  Hyderabad      Near Party Hub   \n",
       "3                20004     tibetan,greek         -1      Near Party Hub   \n",
       "4                20005     cuban,british      Pune       Near Party Hub   \n",
       "\n",
       "  Opening Day of Restaurant  Facebook Popularity Quotient       Endorsed By  \\\n",
       "0                13-07-2010                        78.000  Tier A Celebrity   \n",
       "1                05-09-2011                        89.170      Not Specific   \n",
       "2                12-04-2011                        84.000      Not Specific   \n",
       "3                16-01-2005                        79.380      Not Specific   \n",
       "4                10-11-2008                        84.670      Not Specific   \n",
       "\n",
       "   Instagram Popularity Quotient  Fire Audit  Liquor License Obtained  ...  \\\n",
       "0                         69.000           1                        1  ...   \n",
       "1                         96.000           1                        1  ...   \n",
       "2                         86.000           1                        1  ...   \n",
       "3                         74.400           0                        1  ...   \n",
       "4                         86.460           1                        1  ...   \n",
       "\n",
       "   Overall Restaurant Rating  Live Music Rating  Comedy Gigs Rating  \\\n",
       "0                      6.000                NaN               2.000   \n",
       "1                      9.000              3.000                 NaN   \n",
       "2                      6.000              3.000                 NaN   \n",
       "3                      9.000              6.000                 NaN   \n",
       "4                        NaN                NaN                 NaN   \n",
       "\n",
       "   Value Deals Rating Live Sports Rating Ambience  Lively  Service  \\\n",
       "0                 NaN                NaN        5       2        2   \n",
       "1                 NaN              6.000        7       8        5   \n",
       "2                 NaN                NaN        9       7        5   \n",
       "3                 NaN                NaN        6       7        6   \n",
       "4                 NaN                NaN        4       4        7   \n",
       "\n",
       "   Comfortablility  Privacy  \n",
       "0                6        0  \n",
       "1                1        8  \n",
       "2                1        5  \n",
       "3                4        7  \n",
       "4                7        3  \n",
       "\n",
       "[5 rows x 33 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##display the first five rows of the test dataset.\n",
    "test.head(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The train data size before dropping Id feature is : (3493, 34) \n",
      "The test data size before dropping Id feature is : (500, 33) \n",
      "\n",
      "The train data size after dropping Id feature is : (3493, 34) \n",
      "The test data size after dropping Id feature is : (500, 33) \n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'test1' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 16\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mThe train data size after dropping Id feature is : \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(train\u001b[38;5;241m.\u001b[39mshape)) \n\u001b[0;32m     15\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe test data size after dropping Id feature is : \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(test\u001b[38;5;241m.\u001b[39mshape))\n\u001b[1;32m---> 16\u001b[0m \u001b[43mtest1\u001b[49m \n",
      "\u001b[1;31mNameError\u001b[0m: name 'test1' is not defined"
     ]
    }
   ],
   "source": [
    "#check the numbers of samples and features\n",
    "print(\"The train data size before dropping Id feature is : {} \".format(train.shape))\n",
    "print(\"The test data size before dropping Id feature is : {} \".format(test.shape))\n",
    "\n",
    "#Save the 'Id' column\n",
    "# train_ID = train['Registration Number']\n",
    "# test_ID = test[]\n",
    "\n",
    "#Now drop the  'Id' colum since it's unnecessary for  the prediction process.\n",
    "# train.drop(\"Registration Number\", axis = 1, inplace = True)\n",
    "# test.drop(\"Registration Number\", axis = 1, inplace = True)\n",
    "\n",
    "#check again the data size after dropping the 'Id' variable\n",
    "print(\"\\nThe train data size after dropping Id feature is : {} \".format(train.shape)) \n",
    "print(\"The test data size after dropping Id feature is : {} \".format(test.shape))\n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleanup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Target Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(train['Annual Turnover'] , fit=norm);\n",
    "\n",
    "# Get the fitted parameters used by the function\n",
    "(mu, sigma) = norm.fit(train['Annual Turnover'])\n",
    "print( '\\n mu = {:.2f} and sigma = {:.2f}\\n'.format(mu, sigma))\n",
    "\n",
    "#Now plot the distribution\n",
    "plt.legend(['Normal dist. ($\\mu=$ {:.2f} and $\\sigma=$ {:.2f} )'.format(mu, sigma)],\n",
    "            loc='best')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Annual Turnover distribution')\n",
    "\n",
    "#Get also the QQ-plot\n",
    "fig = plt.figure()\n",
    "res = stats.probplot(train['Annual Turnover'], plot=plt)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We use the numpy fuction log1p which  applies log(1+x) to all elements of the column\n",
    "train[\"Annual Turnover\"] = np.log1p(train[\"Annual Turnover\"])\n",
    "\n",
    "#Check the new distribution \n",
    "sns.distplot(train['Annual Turnover'] , fit=norm);\n",
    "\n",
    "# Get the fitted parameters used by the function\n",
    "(mu, sigma) = norm.fit(train['Annual Turnover'])\n",
    "print( '\\n mu = {:.2f} and sigma = {:.2f}\\n'.format(mu, sigma))\n",
    "\n",
    "#Now plot the distribution\n",
    "plt.legend(['Normal dist. ($\\mu=$ {:.2f} and $\\sigma=$ {:.2f} )'.format(mu, sigma)],\n",
    "            loc='best')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Annual Turnover distribution')\n",
    "\n",
    "#Get also the QQ-plot\n",
    "fig = plt.figure()\n",
    "res = stats.probplot(train['Annual Turnover'], plot=plt)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ntrain = train.shape[0]\n",
    "ntest = test.shape[0]\n",
    "y_train = train\n",
    "all_data = pd.concat((train, test)).reset_index(drop=True)\n",
    "all_data.drop(['Annual Turnover'], axis=1, inplace=True)\n",
    "print(\"all_data size is : {}\".format(all_data.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fix City"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data['City'] = all_data['City'].replace('-1', 'NOCITY')\n",
    "all_data['City'] = all_data['City'].str.upper().str.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "replacement_pattern = r\"^(BANGALORE|BANGLORE|BENGALURU|Asifabadbanglore|Banaglore)$\"  # Consider using raw string for regex\n",
    "\n",
    "# Replace matching values with 'BANGALORE' (case-sensitive)\n",
    "all_data['City'] = all_data['City'].str.replace(replacement_pattern, 'BANGALORE')\n",
    "\n",
    "\n",
    "replacement_pattern = r\"^(BHUBANESHWAR|BHUBANESWAR|BHUBNESHWAR)$\"  # Consider using raw string for regex\n",
    "all_data['City'] = all_data['City'].str.replace(replacement_pattern, 'BHUBANESHWAR')\n",
    "\n",
    "\n",
    "replacement_pattern = r\"^(TRIVANDRUM|THIRUVANANTHAPURAM|THIRUVANANTHAPURAM)$\"  # Consider using raw string for regex\n",
    "all_data['City'] = all_data['City'].str.replace(replacement_pattern, 'TRIVANDRUM')\n",
    "\n",
    "replacement_pattern = r\"^(GURAGAON|GURGAON)$\"  # Consider using raw string for regex\n",
    "all_data['City'] = all_data['City'].str.replace(replacement_pattern, 'GURAGAON')\n",
    "\n",
    "replacement_pattern = r\"^(NEW DEHLI|NEW DELHI)$\"  # Consider using raw string for regex\n",
    "all_data['City'] = all_data['City'].str.replace(replacement_pattern, 'NEW DELHI')\n",
    "\n",
    "\n",
    "replacement_pattern = r\"^(VISAKHAPATNAM|VIZAG|VSAKHAPTTNAM)$\"  # Consider using raw string for regex\n",
    "all_data['City'] = all_data['City'].str.replace(replacement_pattern, 'VIZAG')\n",
    "\n",
    "all_data.loc[all_data['City'].isin((all_data['City'].value_counts()[all_data['City'].value_counts() < 6]).index), 'City'] = 'OTHER'\n",
    "\n",
    "all_data['City'].value_counts().sort_values(ascending=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data_na = (all_data.isnull().sum() / len(all_data)) * 100\n",
    "all_data_na = all_data_na.drop(all_data_na[all_data_na == 0].index).sort_values(ascending=False)[:30]\n",
    "missing_data = pd.DataFrame({'Missing Ratio' :all_data_na})\n",
    "missing_data.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(figsize=(15, 12))\n",
    "plt.xticks(rotation='vertical')\n",
    "sns.barplot(x=all_data_na.index, y=all_data_na)\n",
    "plt.xlabel('Features', fontsize=15)\n",
    "plt.ylabel('Percent of missing values', fontsize=15)\n",
    "plt.title('Percent missing data by feature', fontsize=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Correlation map to see how features are correlated with SalePrice\n",
    "corrmat = train.corr()\n",
    "plt.subplots(figsize=(12,9))\n",
    "sns.heatmap(corrmat, vmax=0.9, square=True,color='blue')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data[\"Live Sports Rating\"] = all_data[\"Live Sports Rating\"].transform(lambda x: x.fillna(x.median()))\n",
    "all_data[\"Live Sports Rating\"].value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data[\"Value Deals Rating\"] = all_data[\"Value Deals Rating\"].transform(lambda x: x.fillna(x.median()))\n",
    "all_data['Value Deals Rating'].value_counts(dropna=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data[\"Comedy Gigs Rating\"] = all_data[\"Comedy Gigs Rating\"].transform(lambda x: x.fillna(x.median()))\n",
    "all_data['Comedy Gigs Rating'].value_counts(dropna=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data[\"Live Music Rating\"] = all_data[\"Live Music Rating\"].transform(lambda x: x.fillna(x.median()))\n",
    "all_data['Live Music Rating'].value_counts(dropna=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data[\"Overall Restaurant Rating\"] = all_data[\"Overall Restaurant Rating\"].transform(lambda x: x.fillna(x.median()))\n",
    "all_data['Overall Restaurant Rating'].value_counts(dropna=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data[\"Facebook Popularity Quotient\"] = all_data[\"Facebook Popularity Quotient\"].transform(lambda x: x.fillna(x.mean()))\n",
    "all_data['Facebook Popularity Quotient'].value_counts(dropna=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data[\"Instagram Popularity Quotient\"] = all_data[\"Instagram Popularity Quotient\"].transform(lambda x: x.fillna(x.mean()))\n",
    "all_data['Instagram Popularity Quotient'].value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data[\"Ambience\"] = all_data[\"Ambience\"].transform(lambda x: x.fillna(x.median()))\n",
    "all_data['Ambience'].value_counts(dropna=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data[\"Resturant Tier\"] = all_data[\"Resturant Tier\"].transform(lambda x: x.fillna(x.median()))\n",
    "all_data['Resturant Tier'].value_counts(dropna=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check remaining missing values if any \n",
    "all_data_na = (all_data.isnull().sum() / len(all_data)) * 100\n",
    "all_data_na = all_data_na.drop(all_data_na[all_data_na == 0].index).sort_values(ascending=False)\n",
    "missing_data = pd.DataFrame({'Missing Ratio' :all_data_na})\n",
    "missing_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### City TE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_replace = [col for col in train.columns if col != 'Annual Turnover']\n",
    "# Update train with corresponding columns from all_data (excluding Annual Turnover)\n",
    "train.update(all_data[:ntrain][columns_to_replace])\n",
    "\n",
    "#train = all_data[:ntrain]\n",
    "import category_encoders as ce\n",
    "encoder = ce.TargetEncoder()\n",
    "train['City_TE'] = encoder.fit_transform(train['City'],np.log1p( train['Annual Turnover']))\n",
    "\n",
    "\n",
    "# Assuming City_encoded in df_train has target means\n",
    "df_target_encoding = train[['City_TE', 'City']]  # Optional: Select relevant columns\n",
    "df_target_encoding = df_target_encoding.drop_duplicates()\n",
    "# # Assuming City_target_encoding exists in df_train_without_duplicates\n",
    "city_target_means = df_target_encoding.set_index('City')['City_TE'].to_dict()\n",
    "print(city_target_means)\n",
    "\n",
    "# # # Example usage on df_test (replace with your actual usage)\n",
    "test  = all_data[ntrain:]\n",
    "\n",
    "test['City_TE'] = test['City'].replace(city_target_means)\n",
    "# # print(test['City_TE'])\n",
    "all_data = pd.concat((train, test))\n",
    "all_data.drop(['Annual Turnover'], axis=1, inplace=True)\n",
    "#all_data.drop(['City'], axis=1, inplace=True)\n",
    "\n",
    "#test\n",
    "all_data\n",
    "\n",
    "# print(train['City_TE'])\n",
    "# df_test.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Restaurant Theme TE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_replace = [col for col in train.columns if col != 'Annual Turnover']\n",
    "# Update train with corresponding columns from all_data (excluding Annual Turnover)\n",
    "train.update(all_data[:ntrain][columns_to_replace])\n",
    "\n",
    "import category_encoders as ce\n",
    "encoder = ce.TargetEncoder()\n",
    "train['Theme_TE'] = encoder.fit_transform(train['Restaurant Theme'],np.log1p( train['Annual Turnover']))\n",
    "\n",
    "\n",
    "# Assuming City_encoded in df_train has target means\n",
    "df_target_encoding = train[['Theme_TE', 'Restaurant Theme']]  # Optional: Select relevant columns\n",
    "df_target_encoding = df_target_encoding.drop_duplicates()\n",
    "# # Assuming City_target_encoding exists in df_train_without_duplicates\n",
    "theme_target_means = df_target_encoding.set_index('Restaurant Theme')['Theme_TE'].to_dict()\n",
    "# print(city_target_means)\n",
    "\n",
    "# # Example usage on df_test (replace with your actual usage)\n",
    "test  = all_data[ntrain:]\n",
    "test['Theme_TE'] = test['Restaurant Theme'].replace(theme_target_means)\n",
    "# print(test['City_TE'])\n",
    "all_data = pd.concat((train, test))\n",
    "all_data.drop(['Annual Turnover'], axis=1, inplace=True)\n",
    "\n",
    "#all_data.drop('Restaurant Theme', axis=1, inplace=True)\n",
    "\n",
    "all_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cuisine Type TE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# columns_to_replace = [col for col in train.columns if col != 'Annual Turnover']\n",
    "# # Update train with corresponding columns from all_data (excluding Annual Turnover)\n",
    "# train.update(all_data[:ntrain][columns_to_replace])\n",
    "\n",
    "# import category_encoders as ce\n",
    "# encoder = ce.TargetEncoder()\n",
    "# train['Cuisine_TE'] = encoder.fit_transform(train['Cuisine'],np.log1p( train['Annual Turnover']))\n",
    "\n",
    "\n",
    "# # Assuming City_encoded in df_train has target means\n",
    "# df_target_encoding = train[['Cuisine_TE', 'Cuisine']]  # Optional: Select relevant columns\n",
    "# df_target_encoding = df_target_encoding.drop_duplicates()\n",
    "# # # Assuming City_target_encoding exists in df_train_without_duplicates\n",
    "# cuisine_target_means = df_target_encoding.set_index('Cuisine')['Cuisine_TE'].to_dict()\n",
    "# # print(city_target_means)\n",
    "\n",
    "# # # Example usage on df_test (replace with your actual usage)\n",
    "# test  = all_data[ntrain:]\n",
    "# test['Cuisine_TE'] = test['Cuisine'].replace(cuisine_target_means)\n",
    "# # print(test['City_TE'])\n",
    "# all_data = pd.concat((train, test))\n",
    "# all_data.drop(['Annual Turnover'], axis=1, inplace=True)\n",
    "\n",
    "# #all_data.drop(['Cuisine'], axis=1, inplace=True)\n",
    "\n",
    "# all_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#all_data = pd.concat([all_data.drop('Cuisine', 1), all_data['Cuisine'].str.get_dummies()], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_data = pd.concat([all_data.drop('Resturent Tier', 1), all_data['Resturent Tier'].str.get_dummies()], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#all_data = pd.concat([all_data.drop('Restaurant Location', 1), all_data['Restaurant Location'].str.get_dummies()], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = pd.get_dummies(all_data,columns=[\"Restaurant Location\"])\n",
    "\n",
    "all_data = pd.get_dummies(all_data,columns=[\"Fire Audit\"])\n",
    "all_data = pd.get_dummies(all_data,columns=[\"Liquor License Obtained\"])\n",
    "all_data = pd.get_dummies(all_data,columns=[\"Situated in a Multi Complex\"])\n",
    "all_data = pd.get_dummies(all_data,columns=[\"Dedicated Parking\"])\n",
    "\n",
    "all_data = pd.get_dummies(all_data,columns=[\"Open Sitting Available\"])\n",
    "all_data = pd.get_dummies(all_data,columns=[\"Restaurant Zomato Rating\"])\n",
    "all_data = pd.get_dummies(all_data,columns=[\"Order Wait Time\"])\n",
    "all_data = pd.get_dummies(all_data,columns=[\"Restaurant City Tier\"])\n",
    "all_data = pd.get_dummies(all_data,columns=[\"Hygiene Rating\"])\n",
    "all_data = pd.get_dummies(all_data,columns=[\"Food Rating\"])\n",
    "all_data = pd.get_dummies(all_data,columns=[\"Overall Restaurant Rating\"])\n",
    "\n",
    "all_data = pd.get_dummies(all_data,columns=[\"Staff Responsivness\"])\n",
    "all_data = pd.get_dummies(all_data,columns=[\"Value for Money\"])\n",
    "all_data = pd.get_dummies(all_data,columns=[\"Live Music Rating\"])\n",
    "all_data = pd.get_dummies(all_data,columns=[\"Comedy Gigs Rating\"])\n",
    "all_data = pd.get_dummies(all_data,columns=[\"Value Deals Rating\"])\n",
    "\n",
    "all_data = pd.get_dummies(all_data,columns=[\"Live Sports Rating\"])\n",
    "all_data = pd.get_dummies(all_data,columns=[\"Ambience\"])\n",
    "all_data = pd.get_dummies(all_data,columns=[\"Lively\"])\n",
    "all_data = pd.get_dummies(all_data,columns=[\"Service\"])\n",
    "all_data = pd.get_dummies(all_data,columns=[\"Comfortablility\"])\n",
    "all_data = pd.get_dummies(all_data,columns=[\"Privacy\"])\n",
    "\n",
    "all_data = pd.get_dummies(all_data,columns=[\"Restaurant Type\"])\n",
    "\n",
    "all_data = pd.get_dummies(all_data,columns=[\"Cuisine\"])\n",
    "all_data = pd.get_dummies(all_data,columns=[\"Endorsed By\"])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# 'Restaurant Type', 'Staff Responsivness',\n",
    "#        'Value for Money', 'Live Music Rating', 'Comedy Gigs Rating',\n",
    "#        'Value Deals Rating', 'Live Sports Rating', 'Ambience', 'Lively',\n",
    "#        'Service', 'Comfortablility', 'Privacy'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# columnsToOHE = ['Cuisine','Restaurant Location',\n",
    "#        'Endorsed By',  'Fire Audit',\n",
    "#        'Liquor License Obtained', 'Situated in a Multi Complex',\n",
    "#        'Dedicated Parking', 'Open Sitting Available', \n",
    "#        'Restaurant Type',  'Restaurant Zomato Rating',\n",
    "#        'Restaurant City Tier', 'Order Wait Time', 'Staff Responsivness',\n",
    "#        'Value for Money', 'Hygiene Rating', 'Food Rating',\n",
    "#        'Overall Restaurant Rating', 'Live Music Rating', 'Comedy Gigs Rating',\n",
    "#        'Value Deals Rating', 'Live Sports Rating', 'Ambience', 'Lively',\n",
    "#        'Service', 'Comfortablility', 'Privacy']\n",
    "\n",
    "# # prefixes = [\"Cuisine\",'Location',\n",
    "# #        'Endorsed',  'Fire',\n",
    "# #        'Liquor', 'MultiComplex',\n",
    "# #        'Parking', 'OpenSitting', \n",
    "# #        'RestaurantType',  'RestaurantZomato',\n",
    "# #        'CityTier', 'OrderWaitTime', 'Staff',\n",
    "# #        'ValueMoney', 'HygieneRating', 'FoodRating',\n",
    "# #        'OverallRating', 'LiveMusic', 'ComedyGigs',\n",
    "# #        'ValueDeals', 'LiveSports', 'Ambience', 'Lively',\n",
    "# #        'Service', 'Comfortablility', 'Privacy']\n",
    "\n",
    "# # for col in columnsToOHE:\n",
    "# #   all_data = pd.get_dummies(all_data, columns=[col])  # Encode each column separately\n",
    "\n",
    "# # pd.get_dummies(all_data , columns=columnsToOHE)\n",
    "# #all_data = pd.get_dummies(all_data,  columns=['Cuisine'])\n",
    "# # pd.get_dummies(all_data, prefix='Fire' , columns=['Fire Audit'])\n",
    "# # pd.get_dummies(all_data, prefix='Fire' , columns=['Fire Audit'])\n",
    "# # pd.get_dummies(all_data, prefix='Fire' , columns=['Fire Audit'])\n",
    "# # pd.get_dummies(all_data, prefix='Fire' , columns=['Fire Audit'])\n",
    "# # pd.get_dummies(all_data, prefix='Fire' , columns=['Fire Audit'])\n",
    "# # pd.get_dummies(all_data, prefix='Fire' , columns=['Fire Audit'])\n",
    "# # pd.get_dummies(all_data, prefix='Fire' , columns=['Fire Audit'])\n",
    "# # pd.get_dummies(all_data, prefix='Fire' , columns=['Fire Audit'])\n",
    "# # pd.get_dummies(all_data, prefix='Fire' , columns=['Fire Audit'])\n",
    "# # pd.get_dummies(all_data, prefix='Fire' , columns=['Fire Audit'])\n",
    "# # pd.get_dummies(all_data, prefix='Fire' , columns=['Fire Audit'])\n",
    "# # pd.get_dummies(all_data, prefix='Fire' , columns=['Fire Audit'])\n",
    "# # pd.get_dummies(all_data, prefix='Fire' , columns=['Fire Audit'])\n",
    "\n",
    "# #all_data = pd.concat([all_data.drop('Fire Audit', 1), all_data['Fire Audit'].str.get_dummies()], 1)\n",
    "# # all_data = pd.concat([all_data.drop('Liquor License Obtained', 1), all_data['Liquor License Obtained'].str.get_dummies()], 1)\n",
    "# # all_data = pd.concat([all_data.drop('Situated in a Multi Complex', 1), all_data['Situated in a Multi Complex'].str.get_dummies()], 1)\n",
    "# # all_data = pd.concat([all_data.drop('Open Sitting Available', 1), all_data['Open Sitting Available'].str.get_dummies()], 1)\n",
    "# # all_data = pd.concat([all_data.drop('Order Wait Time', 1), all_data['Order Wait Time'].str.get_dummies()], 1)\n",
    "# # all_data = pd.concat([all_data.drop('Staff Responsivness', 1), all_data['Staff Responsivness'].str.get_dummies()], 1)\n",
    "# # all_data = pd.concat([all_data.drop('Value for Money', 1), all_data['Value for Money'].str.get_dummies()], 1)\n",
    "# # all_data = pd.concat([all_data.drop('Hygiene Rating', 1), all_data['Hygiene Rating'].str.get_dummies()], 1)\n",
    "# # all_data = pd.concat([all_data.drop('Food Rating', 1), all_data['Food Rating'].str.get_dummies()], 1)\n",
    "# # all_data = pd.concat([all_data.drop('Live Music Rating', 1), all_data['Live Music Rating'].str.get_dummies()], 1)\n",
    "# # all_data = pd.concat([all_data.drop('Comedy Gigs Rating', 1), all_data['Comedy Gigs Rating'].str.get_dummies()], 1)\n",
    "# # all_data = pd.concat([all_data.drop('Value Deals Rating', 1), all_data['Value Deals Rating'].str.get_dummies()], 1)\n",
    "# # all_data = pd.concat([all_data.drop('Ambience', 1), all_data['Ambience'].str.get_dummies()], 1)\n",
    "# # all_data = pd.concat([all_data.drop('Lively', 1), all_data['Lively'].str.get_dummies()], 1)\n",
    "# # all_data = pd.concat([all_data.drop('Service', 1), all_data['Service'].str.get_dummies()], 1)\n",
    "# # all_data = pd.concat([all_data.drop('Comfortablility', 1), all_data['Comfortablility'].str.get_dummies()], 1)\n",
    "# # all_data = pd.concat([all_data.drop('Privacy', 1), all_data['Privacy'].str.get_dummies()], 1)\n",
    "# all_data.columns()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_data = pd.concat([all_data.drop('Endorsed By', 1), all_data['Endorsed By'].str.get_dummies()], 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_data = pd.concat([all_data.drop('Restaurant Type', 1), all_data['Restaurant Type'].str.get_dummies()], 1)\n",
    "# all_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Date Field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-16T13:19:59.753437Z",
     "start_time": "2020-09-16T13:19:59.739592Z"
    }
   },
   "outputs": [],
   "source": [
    "#all_data['Opening Day of Restaurant'] =  pd.to_datetime(all_data['Opening Day of Restaurant'], format='%d-%m-%Y')\n",
    "#print(all_data['Opening Day of Restaurant'].dtype)\n",
    "launch_date = datetime.datetime(2015, 3, 23)\n",
    "# scale days open\n",
    "all_data['Days Open'] = (launch_date - pd.to_datetime(all_data['Opening Day of Restaurant'], format='%d-%m-%Y')).dt.days / 1000\n",
    "all_data['Opening Year'] =  pd.to_datetime(all_data['Opening Day of Restaurant'], format='%d-%m-%Y').dt.year\n",
    "all_data['Opening Month'] =  pd.to_datetime(all_data['Opening Day of Restaurant'], format='%d-%m-%Y').dt.month\n",
    "#all_data['Opening Year'].astype(str)\n",
    "all_data['Opening Month'].value_counts()\n",
    "\n",
    "all_data.drop('Opening Day of Restaurant', axis=1, inplace=True)\n",
    "all_data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Year Month OHE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = pd.get_dummies(all_data,columns=[\"Opening Year\"])\n",
    "all_data = pd.get_dummies(all_data,columns=[\"Opening Month\"])\n",
    "\n",
    "all_data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data.drop(['City'], axis=1, inplace=True)\n",
    "# all_data.drop(['Cuisine'], axis=1, inplace=True)\n",
    "all_data.drop(['Restaurant Theme'], axis=1, inplace=True)\n",
    "all_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = all_data[:ntrain]\n",
    "df_test = all_data[ntrain:]\n",
    "df_train = pd.concat([y_train['Annual Turnover'],df_train], axis=1)\n",
    "df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "# import seaborn as sns\n",
    "# plt.figure(figsize=(15, 7))\n",
    "# sns.heatmap(df_train.corr(), annot=True, vmin=-1, vmax=1, fmt=\".2f\", cmap=\"Spectral\") # Complete the code to get the heatmap of the data\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split Train and Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "train_df, val_df = train_test_split(df_train, test_size=0.2, random_state=42)\n",
    "\n",
    "len(train_df), len(val_df) , len(df_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and Validation Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_targets = train_df['Annual Turnover']\n",
    "# Column to exclude\n",
    "columns_to_exclude = ['Annual Turnover','Registration Number']\n",
    "# Select all columns except 'col2' (using drop with subset and inplace=False)\n",
    "\n",
    "train_inputs  = train_df.drop(columns_to_exclude, axis=1, inplace=False)  # Keep original intact\n",
    "val_inputs = val_df.drop(columns_to_exclude, axis=1, inplace=False)  \n",
    "val_targets = val_df['Annual Turnover']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Initialize the MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "numeric = ['Facebook Popularity Quotient', 'Instagram Popularity Quotient',\n",
    "      'Days Open'\n",
    "]\n",
    "\n",
    "# Fit the MinMaxScaler to the training data\n",
    "scaler.fit(train_inputs)\n",
    "\n",
    "# Use the scaler to transform the training and test sets\n",
    "train_inputs[numeric] = scaler.fit_transform(train_inputs[numeric])\n",
    "val_inputs[numeric] = scaler.fit_transform(val_inputs[numeric])\n",
    "# X_test_scaled = scaler.transform(val_inputs)\n",
    "df_test[numeric]= scaler.fit_transform(df_test[numeric])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Try Different Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Change Column Names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "train_inputs = train_inputs.rename(columns = lambda x:re.sub('[^A-Za-z0-9_]+', '', x))\n",
    "val_inputs = val_inputs.rename(columns = lambda x:re.sub('[^A-Za-z0-9_]+', '', x))\n",
    "df_test = df_test.rename(columns = lambda x:re.sub('[^A-Za-z0-9_]+', '', x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "linreg_model = LinearRegression()\n",
    "linreg_model.fit(train_inputs, train_targets)\n",
    "LinearRegression()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_preds = linreg_model.predict(train_inputs)\n",
    "val_preds = linreg_model.predict(val_inputs)\n",
    "\n",
    "#row_61946 = val_inputs.loc[df_train['Registration Number'] == 61946]\n",
    "#print(row_61946)\n",
    "#val_preds_RN = linreg_model.predict(row_61946)\n",
    "#validation_mse_RN = np.sqrt(mean_squared_error([22000000], val_preds_RN))\n",
    "#print(validation_mse_RN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_rmse_linreg = np.sqrt(mean_squared_error(train_preds, train_targets))\n",
    "test_rmse_linreg = np.sqrt(mean_squared_error(val_preds, val_targets))\n",
    "print(f'Train RMSE Linear Reg: {train_rmse_linreg:n}')\n",
    "print(f'Test RMSE Linear Reg: {test_rmse_linreg:n}')\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ridge Linear Regression\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "\n",
    "params_ridge = {\n",
    "    'alpha' : [.01, .1, .5, .7, .9, .95, .99, 1, 5, 10, 20],\n",
    "    'fit_intercept' : [True, False],\n",
    "#    'normalize' : [True,False],\n",
    "    'solver' : ['svd', 'cholesky', 'lsqr', 'sparse_cg', 'sag', 'saga']\n",
    "}\n",
    "\n",
    "ridge_model = Ridge()\n",
    "ridge_regressor = GridSearchCV(ridge_model, params_ridge, scoring='neg_root_mean_squared_error', cv=5, n_jobs=-1)\n",
    "ridge_regressor.fit(train_inputs, train_targets)\n",
    "# print(f'Optimal alpha: {ridge_regressor.best_params_[\"alpha\"]:.2f}')\n",
    "# print(f'Optimal fit_intercept: {ridge_regressor.best_params_[\"fit_intercept\"]}')\n",
    "# #print(f'Optimal normalize: {ridge_regressor.best_params_[\"normalize\"]}')\n",
    "# print(f'Optimal solver: {ridge_regressor.best_params_[\"solver\"]}')\n",
    "# print(f'Best score: {ridge_regressor.best_score_}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ridge_model = Ridge(alpha=ridge_regressor.best_params_[\"alpha\"], fit_intercept=ridge_regressor.best_params_[\"fit_intercept\"], \n",
    "                    solver=ridge_regressor.best_params_[\"solver\"])\n",
    "ridge_model.fit(train_inputs, train_targets)\n",
    "train_preds = ridge_model.predict(train_inputs)\n",
    "val_preds = ridge_model.predict(val_inputs)\n",
    "print('Train r2 score: ', r2_score(train_preds, train_targets))\n",
    "print('Test r2 score: ', r2_score(val_targets, val_preds))\n",
    "train_rmse_ridge = np.sqrt(mean_squared_error(train_preds, train_targets))\n",
    "test_rmse_ridge = np.sqrt(mean_squared_error(val_preds, val_targets))\n",
    "print(f'Train RMSE Ridge: {train_rmse_ridge:n}')\n",
    "print(f'Test RMSE Ridge: {test_rmse_ridge:n}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lasso Model Feature Importance\n",
    "lasso_feature_coef = pd.Series(index = train_inputs.columns, data = np.abs(ridge_model.coef_))\n",
    "lasso_feature_coef.sort_values().plot(kind = 'bar', figsize = (30,5));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lasso Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_lasso = {\n",
    "    'alpha' : [.01, .1, .5, .7, .9, .95, .99, 1, 5, 10, 20],\n",
    "    'fit_intercept' : [True, False],\n",
    "}\n",
    "\n",
    "lasso_model = Lasso()\n",
    "lasso_regressor = GridSearchCV(lasso_model, params_lasso, scoring='neg_root_mean_squared_error', cv=5, n_jobs=-1)\n",
    "lasso_regressor.fit(train_inputs, train_targets)\n",
    "print(f'Optimal alpha: {lasso_regressor.best_params_[\"alpha\"]:.2f}')\n",
    "print(f'Optimal fit_intercept: {lasso_regressor.best_params_[\"fit_intercept\"]}')\n",
    "print(f'Best score: {lasso_regressor.best_score_}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lasso_model = Lasso(alpha=lasso_regressor.best_params_[\"alpha\"], fit_intercept=lasso_regressor.best_params_[\"fit_intercept\"], \n",
    "                    )\n",
    "lasso_model.fit(train_inputs, train_targets)\n",
    "train_preds = lasso_model.predict(train_inputs)\n",
    "val_preds = lasso_model.predict(val_inputs)\n",
    "print('Train r2 score: ', r2_score(train_preds, train_targets))\n",
    "print('Test r2 score: ', r2_score(val_targets, val_preds))\n",
    "train_rmse_lasso = np.sqrt(mean_squared_error(train_preds, train_targets))\n",
    "test_rmse_lasso = np.sqrt(mean_squared_error(val_targets, val_preds))\n",
    "print(f'Train RMSE: {train_rmse_lasso:.4f}')\n",
    "print(f'Test RMSE: {test_rmse_lasso:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lasso Model Feature Importance\n",
    "lasso_feature_coef = pd.Series(index = train_inputs.columns, data = np.abs(lasso_model.coef_))\n",
    "lasso_feature_coef.sort_values().plot(kind = 'bar', figsize = (30,5));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XG Boost Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBRegressor, plot_importance\n",
    "# Scale the data using StandardScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "params_xgb = {\n",
    "    'learning_rate': [.1, .5, .7, .9, .95, .99, 1],\n",
    "    'colsample_bytree': [.3, .4, .5, .6],\n",
    "    'max_depth': [4],\n",
    "    'alpha': [3],\n",
    "    'subsample': [.5],\n",
    "    'n_estimators': [30, 70, 100, 200]\n",
    "}\n",
    "\n",
    "xgb_model = XGBRegressor()\n",
    "xgb_regressor = GridSearchCV(xgb_model, params_xgb, scoring='neg_root_mean_squared_error', cv = 10, n_jobs = -1)\n",
    "xgb_regressor.fit(train_inputs, train_targets)\n",
    "print(f'Optimal lr: {xgb_regressor.best_params_[\"learning_rate\"]}')\n",
    "print(f'Optimal colsample_bytree: {xgb_regressor.best_params_[\"colsample_bytree\"]}')\n",
    "print(f'Optimal n_estimators: {xgb_regressor.best_params_[\"n_estimators\"]}')\n",
    "print(f'Best score: {xgb_regressor.best_score_}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_model = XGBRegressor(learning_rate=xgb_regressor.best_params_[\"learning_rate\"], \n",
    "                         colsample_bytree=xgb_regressor.best_params_[\"colsample_bytree\"], \n",
    "                         max_depth=4, alpha=3, subsample=.5, \n",
    "                         n_estimators=xgb_regressor.best_params_[\"n_estimators\"], n_jobs=-1)\n",
    "xgb_model.fit(train_inputs, train_targets)\n",
    "\n",
    "train_preds = xgb_model.predict(train_inputs)\n",
    "val_preds = xgb_model.predict(val_inputs)\n",
    "print('Train r2 score: ', r2_score(train_preds, train_targets))\n",
    "print('Test r2 score: ', r2_score(val_targets, val_preds))\n",
    "train_rmse_xgb = np.sqrt(mean_squared_error(train_preds, train_targets))\n",
    "test_rmse_xgb = np.sqrt(mean_squared_error(val_targets, val_preds))\n",
    "print(f'Train RMSE XGB: {train_rmse_xgb:n}')\n",
    "print(f'Test RMSE XGB: {test_rmse_xgb:n}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# XG Forest Model Feature Importance\n",
    "rf_feature_importance = pd.Series(index = train_inputs.columns, data = np.abs(xgb_model.feature_importances_))\n",
    "n_features = (rf_feature_importance>0).sum()\n",
    "print(f'{n_features} features with reduction of {(1-n_features/len(rf_feature_importance))*100:2.2f}%')\n",
    "rf_feature_importance.sort_values().plot(kind = 'bar', figsize = (30,5));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest Regressor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# params_rf = {\n",
    "#     'max_depth': [10, 30, 35, 50, 65, 75, 100],\n",
    "#     'max_features': [.3, .4, .5, .6],\n",
    "#     'min_samples_leaf': [3, 4, 5],\n",
    "#     'min_samples_split': [8, 10, 12],\n",
    "#     'n_estimators': [30, 50, 100, 200]\n",
    "# }\n",
    "\n",
    "# rf = RandomForestRegressor()\n",
    "# rf_regressor = GridSearchCV(rf, params_rf, scoring='neg_root_mean_squared_error', cv = 10, n_jobs = -1)\n",
    "# rf_regressor.fit(train_inputs, train_targets)\n",
    "# print(f'Optimal depth: {rf_regressor.best_params_[\"max_depth\"]}')\n",
    "# print(f'Optimal max_features: {rf_regressor.best_params_[\"max_features\"]}')\n",
    "# print(f'Optimal min_sample_leaf: {rf_regressor.best_params_[\"min_samples_leaf\"]}')\n",
    "# print(f'Optimal min_samples_split: {rf_regressor.best_params_[\"min_samples_split\"]}')\n",
    "# print(f'Optimal n_estimators: {rf_regressor.best_params_[\"n_estimators\"]}')\n",
    "# print(f'Best score: {rf_regressor.best_score_}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rf_model = RandomForestRegressor(max_depth=rf_regressor.best_params_[\"max_depth\"], \n",
    "#                                  max_features=rf_regressor.best_params_[\"max_features\"], \n",
    "#                                  min_samples_leaf=rf_regressor.best_params_[\"min_samples_leaf\"], \n",
    "#                                  min_samples_split=rf_regressor.best_params_[\"min_samples_split\"], \n",
    "#                                  n_estimators=rf_regressor.best_params_[\"n_estimators\"], \n",
    "#                                  n_jobs=-1, oob_score=True)\n",
    "# rf_model.fit(train_inputs, train_targets)\n",
    "# train_preds = rf_model.predict(train_inputs)\n",
    "# val_preds = rf_model.predict(val_inputs)\n",
    "# print('Train r2 score: ', r2_score(train_preds, train_targets))\n",
    "# print('Test r2 score: ', r2_score(val_preds,val_targets ))\n",
    "# train_rmse = np.sqrt(mean_squared_error(train_preds, train_targets))\n",
    "# test_rmse = np.sqrt(mean_squared_error(val_targets, val_preds))\n",
    "# print(f'Train RMSE: {train_rmse:n}')\n",
    "# print(f'Test RMSE: {test_rmse:n}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Random Forest Model Feature Importance\n",
    "# rf_feature_importance = pd.Series(index = train_inputs.columns, data = np.abs(rf_model.feature_importances_))\n",
    "# n_features = (rf_feature_importance>0).sum()\n",
    "# print(f'{n_features} features with reduction of {(1-n_features/len(rf_feature_importance))*100:2.2f}%')\n",
    "# rf_feature_importance.sort_values().plot(kind = 'bar', figsize = (13,5));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### With RandomizedSearch instead of GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from scipy.stats import uniform, randint\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "params = {\n",
    "    \"n_estimators\": randint(10,1000),\n",
    "    \"max_depth\": randint(1,10),\n",
    "    \"min_samples_split\": uniform(0.1,0.8),\n",
    "    'max_features':['auto', 'sqrt', 'log2']\n",
    "}\n",
    "\n",
    "rf = RandomForestRegressor()\n",
    "rf_regressor = RandomizedSearchCV(rf, params, scoring='neg_root_mean_squared_error', n_iter=100, cv = 6,  return_train_score=True, verbose=3, n_jobs=-1)\n",
    "rf_regressor.fit(train_inputs, train_targets)\n",
    "# print(f'Optimal depth: {rf_regressor.best_params_[\"max_depth\"]}')\n",
    "# print(f'Optimal max_features: {rf_regressor.best_params_[\"max_features\"]}')\n",
    "# print(f'Optimal min_sample_leaf: {rf_regressor.best_params_[\"min_samples_leaf\"]}')\n",
    "# print(f'Optimal min_samples_split: {rf_regressor.best_params_[\"min_samples_split\"]}')\n",
    "# print(f'Optimal n_estimators: {rf_regressor.best_params_[\"n_estimators\"]}')\n",
    "# print(f'Best score: {rf_regressor.best_score_}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_model_random = RandomForestRegressor(max_depth=rf_regressor.best_params_[\"max_depth\"], \n",
    "                                 max_features=rf_regressor.best_params_[\"max_features\"], \n",
    "                                 min_samples_split=rf_regressor.best_params_[\"min_samples_split\"], \n",
    "                                 n_estimators=rf_regressor.best_params_[\"n_estimators\"], \n",
    "                                 n_jobs=-1, oob_score=True)\n",
    "rf_model_random.fit(train_inputs, train_targets)\n",
    "train_preds = rf_model_random.predict(train_inputs)\n",
    "val_preds = rf_model_random.predict(val_inputs)\n",
    "print('Train r2 score: ', r2_score(train_preds, train_targets))\n",
    "print('Test r2 score: ', r2_score(val_preds,val_targets ))\n",
    "train_rmse_rf_random = np.sqrt(mean_squared_error(train_preds, train_targets))\n",
    "test_rmse_rf_random = np.sqrt(mean_squared_error(val_targets, val_preds))\n",
    "print(f'Train RMSE: {train_rmse_rf_random:n}')\n",
    "print(f'Test RMSE: {test_rmse_rf_random:n}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Random Forest Model Feature Importance\n",
    "rf_feature_importance = pd.Series(index = train_inputs.columns, data = np.abs(rf_model_random.feature_importances_))\n",
    "n_features = (rf_feature_importance>0).sum()\n",
    "print(f'{n_features} features with reduction of {(1-n_features/len(rf_feature_importance))*100:2.2f}%')\n",
    "rf_feature_importance.sort_values().plot(kind = 'bar', figsize = (20,5));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KNN Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "\n",
    "params_knn = {\n",
    "    'n_neighbors' : [3, 5, 7, 9, 11],\n",
    "}\n",
    "\n",
    "knn_model = KNeighborsRegressor()\n",
    "knn_regressor = GridSearchCV(knn_model, params_knn, scoring='neg_root_mean_squared_error', cv=10, n_jobs=-1)\n",
    "knn_regressor.fit(train_inputs, train_targets)\n",
    "print(f'Optimal neighbors: {knn_regressor.best_params_[\"n_neighbors\"]}')\n",
    "print(f'Best score: {knn_regressor.best_score_}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_model = KNeighborsRegressor(n_neighbors=knn_regressor.best_params_[\"n_neighbors\"])\n",
    "knn_model.fit(train_inputs, train_targets)\n",
    "train_preds = knn_model.predict(train_inputs)\n",
    "val_preds = knn_model.predict(val_inputs)\n",
    "print('Train r2 score: ', r2_score(train_preds, train_targets))\n",
    "print('Test r2 score: ', r2_score(val_targets, val_preds))\n",
    "train_rmse_knn = np.sqrt(mean_squared_error(train_preds, train_targets))\n",
    "test_rmse_knn = np.sqrt(mean_squared_error(val_targets, val_preds))\n",
    "print(f'Train RMSE: {train_rmse_knn:n}')\n",
    "print(f'Test RMSE: {test_rmse_knn:n}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Light GBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import lightgbm as lgb\n",
    "from scipy.stats import uniform, randint\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "params = {\n",
    "    \"learning_rate\": uniform(0.001,1),\n",
    "    \"n_estimators\": randint(100,1000),\n",
    "    \"max_depth\": randint(1,10),\n",
    "    \"colsample_bytree\": uniform(0.1,0.8),\n",
    "    \"reg_alpha\": [0.0001,0.001,0.01,0.1,1,10],\n",
    "    \"reg_lambda\": [0.0001,0.001,0.01,0.1,1,10]\n",
    "}\n",
    "\n",
    "lgb_model = lgb.LGBMRegressor()\n",
    "lgb_regressor = RandomizedSearchCV(lgb_model, params, cv=6, n_iter=100, scoring='neg_mean_squared_error', return_train_score=True, verbose=3, n_jobs=-1)\n",
    "lgb_regressor.fit(train_inputs, train_targets)\n",
    "print(f'Optimal lr: {lgb_regressor.best_params_[\"learning_rate\"]}')\n",
    "print(f'Optimal colsample_bytree: {lgb_regressor.best_params_[\"colsample_bytree\"]}')\n",
    "print(f'Optimal n_estimators: {lgb_regressor.best_params_[\"n_estimators\"]}')\n",
    "print(f'Best score: {lgb_regressor.best_score_}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgb_model = lgb.LGBMRegressor(learning_rate=lgb_regressor.best_params_[\"learning_rate\"], \n",
    "                         colsample_bytree=lgb_regressor.best_params_[\"colsample_bytree\"], \n",
    "                         max_depth=4, alpha=3, subsample=.5, \n",
    "                         n_estimators=lgb_regressor.best_params_[\"n_estimators\"], n_jobs=-1)\n",
    "lgb_model.fit(train_inputs, train_targets)\n",
    "train_preds = lgb_model.predict(train_inputs)\n",
    "val_preds = lgb_model.predict(val_inputs)\n",
    "print('Train r2 score: ', r2_score(train_preds, train_targets))\n",
    "print('Test r2 score: ', r2_score(val_targets, val_preds))\n",
    "train_rmse_lgb = np.sqrt(mean_squared_error(train_preds, train_targets))\n",
    "test_rmse_lgb = np.sqrt(mean_squared_error(val_targets, val_preds))\n",
    "print(f'Train RMSE XGB: {train_rmse_lgb:n}')\n",
    "print(f'Test RMSE XGB: {test_rmse_lgb:n}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# XG Forest Model Feature Importance\n",
    "rf_feature_importance = pd.Series(index = train_inputs.columns, data = np.abs(lgb_model.feature_importances_))\n",
    "n_features = (rf_feature_importance>0).sum()\n",
    "print(f'{n_features} features with reduction of {(1-n_features/len(rf_feature_importance))*100:2.2f}%')\n",
    "rf_feature_importance.sort_values().plot(kind = 'bar', figsize = (20,5));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stacking Regressor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stack 1 - Ridge Lasso RF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_model_en = RandomForestRegressor(max_depth=200, max_features=0.4, min_samples_leaf=3, \n",
    "                                 min_samples_split=6, n_estimators=30, n_jobs=-1, oob_score=True)\n",
    "rf_model_en.fit(train_inputs, train_targets)\n",
    "train_preds = rf_model_en.predict(train_inputs)\n",
    "val_preds = rf_model_en.predict(val_inputs)\n",
    "print('Train r2 score: ', r2_score(train_preds, train_targets))\n",
    "print('Test r2 score: ', r2_score(val_targets, val_preds))\n",
    "train_rmse_stack1 = np.sqrt(mean_squared_error(train_preds, train_targets))\n",
    "test_rmse_stack1 = np.sqrt(mean_squared_error(val_targets, val_preds))\n",
    "print(f'Train RMSE: {train_rmse_stack1:.4f}')\n",
    "print(f'Test RMSE: {test_rmse_stack1:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import mean\n",
    "from numpy import std\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import RepeatedKFold\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import StackingRegressor\n",
    "from matplotlib import pyplot\n",
    "\n",
    "# # get a stacking ensemble of models\n",
    "# def get_stacking():\n",
    "#     # define the base models\n",
    "#     base_models = list()\n",
    "#     base_models.append(('ridge', ridge_model))\n",
    "#     base_models.append(('lasso', lasso_model))\n",
    "#     base_models.append(('rf', rf_model_en))\n",
    "#     # define meta learner model\n",
    "#     learner = LinearRegression()\n",
    "#     # define the stacking ensemble\n",
    "#     model = StackingRegressor(estimators=base_models, final_estimator=learner, cv=10)\n",
    "#     return model\n",
    " \n",
    "# # get a list of models to evaluate\n",
    "# def get_models():\n",
    "#     models = dict()\n",
    "#     models['ridge'] = ridge_model\n",
    "#     models['lasso'] = lasso_model\n",
    "#     models['rf_en'] = rf_model_en\n",
    "#     models['stacking'] = get_stacking()\n",
    "#     return models\n",
    " \n",
    "# # evaluate a given model using cross-validation\n",
    "# def evaluate_model(model, X, y):\n",
    "#     cv = RepeatedKFold(n_splits=10, n_repeats=5, random_state=19)\n",
    "#     scores = cross_val_score(model, X, y, scoring='neg_mean_absolute_error', cv=cv, n_jobs=-1, error_score='raise')\n",
    "#     return scores\n",
    "\n",
    "# # get the models to evaluate\n",
    "# models = get_models()\n",
    "# # evaluate the models and store results\n",
    "# results, names = list(), list()\n",
    "# for name, model in models.items():\n",
    "#     scores = evaluate_model(model, train_inputs, train_targets)\n",
    "#     results.append(scores)\n",
    "#     names.append(name)\n",
    "#     print(f'{name} {mean(scores):.3f} {std(scores):.3f}')\n",
    "# # plot model performance for comparison\n",
    "# pyplot.boxplot(results, labels=names, showmeans=True)\n",
    "# pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the base models\n",
    "base_models = list()\n",
    "base_models.append(('ridge', ridge_model))\n",
    "base_models.append(('lasso', lasso_model))\n",
    "base_models.append(('xgb', xgb_model))\n",
    "base_models.append(('lgm', lgb_model))\n",
    "# define meta learner model\n",
    "learner = LinearRegression()\n",
    "# define the stacking ensemble\n",
    "stack1 = StackingRegressor(estimators=base_models, final_estimator=learner, cv=10)\n",
    "# fit the model on all available data\n",
    "stack1.fit(train_inputs, train_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_preds = stack1.predict(train_inputs)\n",
    "val_preds = stack1.predict(val_inputs)\n",
    "print('Train r2 score: ', r2_score(train_preds, train_targets))\n",
    "print('Test r2 score: ', r2_score(val_targets, val_preds))\n",
    "train_rmse_stack1 = np.sqrt(mean_squared_error(train_preds, train_targets))\n",
    "test_rmse_stack1 = np.sqrt(mean_squared_error(val_targets, val_preds))\n",
    "print(f'Train RMSE: {train_rmse_stack1:.4f}')\n",
    "print(f'Test RMSE: {test_rmse_stack1:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stack 2 - RF 1 / 2 / 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the base models\n",
    "base_model = list()\n",
    "base_model.append(('rf1', rf_model_random))\n",
    "base_model.append(('rf2', rf_model_en))\n",
    "base_model.append(('rf3', RandomForestRegressor(max_depth=8, max_features=0.1, min_samples_leaf=3, \n",
    "                                                min_samples_split=2, n_estimators=250, n_jobs=-1, oob_score=False)))\n",
    "# define meta learner model\n",
    "learner = LinearRegression()\n",
    "# define the stacking ensemble\n",
    "stack2 = StackingRegressor(estimators=base_model, final_estimator=learner, cv=10)\n",
    "# fit the model on all available data\n",
    "stack2.fit(train_inputs, train_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_preds = stack2.predict(train_inputs)\n",
    "val_preds = stack2.predict(val_inputs)\n",
    "print('Train r2 score: ', r2_score(train_preds, train_targets))\n",
    "print('Test r2 score: ', r2_score(val_targets, val_preds))\n",
    "train_rmse_stack2 = np.sqrt(mean_squared_error(train_preds, train_targets))\n",
    "test_rmse_stack2 = np.sqrt(mean_squared_error(val_targets, val_preds))\n",
    "print(f'Train RMSE: {train_rmse_stack2:.4f}')\n",
    "print(f'Test RMSE: {test_rmse_stack2:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Comparision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tabulate import tabulate \n",
    "#https://www.geeksforgeeks.org/display-the-pandas-dataframe-in-table-style/\n",
    "dict = {'Models'     : ['Linear Regression','Ridge Linear Regression','Lasso','XGB','RF -Random','KNN','LightGBM','Stack1','Stack2' ],\n",
    "        'Train RMSE' : [train_rmse_linreg,train_rmse_ridge,train_rmse_lasso,train_rmse_xgb,train_rmse_rf_random,train_rmse_knn,train_rmse_lgb,train_rmse_stack1,train_rmse_stack2],\n",
    "        'Test RMSE'  : [test_rmse_linreg,test_rmse_ridge,test_rmse_lasso,test_rmse_xgb,test_rmse_rf_random,test_rmse_knn,test_rmse_lgb,test_rmse_stack1,test_rmse_stack2]  \n",
    "        }\n",
    "df = pd.DataFrame(dict)\n",
    "print(tabulate(df, headers = 'keys', tablefmt = 'psql')) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading the Test Data and using the Test Data to Predict the target Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-16T13:27:27.995077Z",
     "start_time": "2020-09-16T13:27:27.913992Z"
    }
   },
   "outputs": [],
   "source": [
    "df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-16T13:28:33.529118Z",
     "start_time": "2020-09-16T13:28:33.500936Z"
    }
   },
   "outputs": [],
   "source": [
    "## Using the model built on the Training set to predict on the Test Set\n",
    "df_test_for_prediction = df_test.drop('RegistrationNumber',axis=1, inplace=False)\n",
    "prediction_linreg = linreg_model.predict(df_test_for_prediction)\n",
    "#prediction_rf = rf_model.predict(df_test_for_prediction)\n",
    "\n",
    "#prediction_rf_random = rf_model_random.predict(df_test_for_prediction)\n",
    "\n",
    "prediction_xgb_model = xgb_model.predict(df_test_for_prediction)\n",
    "prediction_ridge_model = ridge_model.predict(df_test_for_prediction)\n",
    "prediction_knn_model = knn_model.predict(df_test_for_prediction)\n",
    "\n",
    "prediction_lasso_model = lasso_model.predict(df_test_for_prediction)\n",
    "prediction_stack1_model = stack1.predict(df_test_for_prediction)\n",
    "prediction_stack2_model = stack2.predict(df_test_for_prediction)\n",
    "\n",
    "prediction_lgb_regressor = lgb_regressor.predict(df_test_for_prediction)\n",
    "\n",
    "\n",
    "prediction_ridge_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving the output in a data frame and then exporting it to a '.csv' file with the appropriate 'Registration Number'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-16T13:38:44.072923Z",
     "start_time": "2020-09-16T13:38:44.055736Z"
    }
   },
   "outputs": [],
   "source": [
    "solution_df = pd.DataFrame(df_test['RegistrationNumber'])\n",
    "solution_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-16T13:40:01.957550Z",
     "start_time": "2020-09-16T13:40:01.935844Z"
    }
   },
   "outputs": [],
   "source": [
    "#solution_df['Annual Turnover'] = prediction_rf\n",
    "\n",
    "#solution_df['Annual Turnover'] = np.expm1(prediction_rf).astype('int64')\n",
    "\n",
    "#solution_df['Annual Turnover'] = np.expm1(prediction_rf_random).astype('int64')\n",
    "\n",
    "\n",
    "#solution_df['Annual Turnover'] = np.expm1(prediction_xgb_model).astype('int64')\n",
    "\n",
    "#solution_df['Annual Turnover'] = (np.expm1(np.square(prediction_ridge_model)).astype('int64') )         #BEST\n",
    "\n",
    "#solution_df['Annual Turnover'] = np.expm1(prediction_knn_model).astype('int64')\n",
    "\n",
    "#solution_df['Annual Turnover'] = np.expm1(prediction_lasso_model).astype('int64')\n",
    "\n",
    "#solution_df['Annual Turnover'] = np.expm1(prediction_stack1_model).astype('int64')\n",
    "\n",
    "#solution_df['Annual Turnover'] = np.expm1(prediction_stack2_model).astype('int64')\n",
    "#solution_df['Annual Turnover'] = np.expm1(prediction_stack2_model).astype('int64')\n",
    "\n",
    "\n",
    "#solution_df['Annual Turnover'] = (np.expm1(np.square(prediction_lgb_regressor)).astype('int64') )   \n",
    "\n",
    "solution_df['Annual Turnover'] = (np.expm1(prediction_stack2_model).astype('int64') )   \n",
    "\n",
    "\n",
    "solution_df = solution_df.rename(columns={'RegistrationNumber': 'Registration Number'})\n",
    "solution_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-16T14:08:38.203042Z",
     "start_time": "2020-09-16T14:08:38.196084Z"
    }
   },
   "outputs": [],
   "source": [
    "## Setting the directory to export the file as a '.csv'\n",
    "\n",
    "import os\n",
    "os.chdir(folderPath + '/output/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-16T14:07:06.368811Z",
     "start_time": "2020-09-16T14:07:06.345683Z"
    }
   },
   "outputs": [],
   "source": [
    "## Exporting the data frame to a '.csv' file and setting the index = False as we do want the index\n",
    "\n",
    "solution_df.to_csv('Submission_stack2_log.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# END"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
